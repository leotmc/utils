## 知识图谱、意图识别方案

[toc]

## 1. 图谱构建

## 2. 意图识别

​		意图识别的整体流程主要包括如下几个部分：

<img src="/Users/leotmc/Business Agendas/ft/intent_recognization.png" alt="intent_recognization" style="zoom:50%;" />

​		对于用户输入的query，首先通过实体识别模型识别出里面的实体或实体提及(mention)；然后基于实体链接模型将实体提及映射到图谱里的标准实体上；同时也会基于意图分类模型对query做分类，得到query所属的意图类别；最后槽位填充模块基于标准实体和意图类别确定最终意图。

### 2.1意图分类

除了识别出query中的实体外，还需要识别出query属于哪个意图，是"找楼栋"还是"找企业"等等。基于MacBERT、NoisyTune以及RecAdam等对query做意图的分类。

- MacBERT

  ​		自谷歌提出BERT以来，BERT在多个nlp任务中都带来了显著的效果提升。BERT即双向掩码语言模型，通过掩码以及双向编码机制并结合下个句子预测的目标函数，在大量语料上训练模型。训练好的模型可用于对下游任务做fine-tuning。BERT在预训练阶段以及用于下游任务fine-tuning阶段流程具体如下图所示:

  **补充图片**

  MacBERT作为BERT的一个变种，其相对于BERT的改进点主要有以下几点：

  - 替换下一个句子预测的目标函数(NSP)为句子顺序预测的目标函数(NOP)。
  - 使用整词和ngram来选择被掩码的部分。原始BERT中使用word-piece的方式来切分文本，由于word-piece的分词方式会将一个词分成几个片段，如果直接从切分的片段中选择掩码，会造成只掩码一个词的一部分，使用整词掩码的方式可以避免这种问题。采用ngram的掩码方式，模型在学习的时候预测的是整个ngram片段而非单个词，使得模型学习难度增加，强迫模型更好地理解整个句子。
  - 使用同义词而非[MASK]来替换被掩码的部分。在原始BERT中训练的时候使用[MASK]符号来替换被掩码的部分，然而在预测的时候，句子中没有[MASK]符号，这种训练和预测的不一致会造成效果退化。MacBERT使用同义词而非[MASK]符号来替换被掩码的部分，有效的解决了模型训练和预测不一致的问题。

- NoisyTune

  ​		由于预训练语言模型可能在预训练任务及数据上存在过拟合，直接使用预训练的语言模型对下游任务做fine-tuning可能会得到局部最优的效果，NoisyTune基于预训练语言模型不同参数矩阵的特性，在对下游任务做fine-tuning前，对不同的参数矩阵添加不同的噪声，即：

  ​																			$$\hat W_{i}=W_{i} + U(-\frac{\lambda}{2},\frac{\lambda}{2}) * std(W_{i})$$

  ​		预训练语言模型的参数矩阵为$[W_1,W_{2},\dots,W_{N}]$，$N$是参数矩阵的种类数，$\hat W_{i}$为$W_{i}$加入噪声后的值，$U(a,b)$为从$a$到$b$的均匀分布，$\lambda$为控制噪声力度的超参数，$std$为标准差。

  ​		通过对参数矩阵添加噪声，使用预训练语言模型对下游任务做fine-tuning的时候可以取得更好的效果。直接基于预训练的语言模型对下游任务做fine-tuning和使用NoisyTune对下游任务做fine-tuning的对比示意图如下图所示：

  ​	**补充图片**

- RecAdam

  ​		在做迁移学习的时候，常常用到两种方式：1. 顺序迁移学习，首先以无监督的方式在大规模语料上训练语言模型，然后基于训练好的语言模型(预训练的语言模型)对下游的任务做fine-tuning；2. 多任务学习，多任务学习是指同时在多个任务中学习并训练模型，通过共享在多个任务中学到的知识来提升模型的效果。多任务学习的损失函数如下：

  ​																				$$loss_{M} = \lambda loss_{T} + (1-\lambda)loss_{S}$$

  其中$loss_{T}$为下游任务的损失函数，$loss_{S}$为源任务的损失函数(在这里即为预训练语言模型的损失函数)。

  ​		在使用预训练语言模型对下游任务做fine-tuning的时候，往往会存在退化遗忘的问题，即在fine-tuning的时候模型会忘掉预训练模型里学到的知识，因此在用于下游任务做迁移学习的时候往往只能得到局部最优。RecAdan通过预训练模拟机制和目标平移技术，有效地缓解了退化遗忘的问题。

  预训练模拟：

  ​		在多任务学习中往往会用到多个不同任务的数据源，由于预训练模型训练中用到的语料往往很大，训练时间长，不易直接拿来和下游任务一起做多任务学习。RecAdam通过引入一些假设条件，简化语言模型的损失函数，使得在用语言模型和下游任务做多任务学习的时候，不实用语言模型的训练语料，仅仅使用预训练的语言模型的参数即可和下游任务一起完成多任务学习。

  ​		语言模型的损失函数为：

  ​																				$$loss_{S}=-logP(\theta|D_{s})$$

  通过引入拉普拉斯近似、费雪信息矩阵以及费雪信息矩阵对角元素的独立同分布假设，上述损失函数可以简化为：

  ​																			$$loss_{S}=\frac{1}{2}\lambda\sum_{i}(\theta_{i}-\theta_{i}^{*})^2$$ 

  其中，$\theta_{i}$为用预训练模型对下游任务做fine-tuning时的模型参数，$\theta_{i}^{*}$为预训练模型的模型参数。

  目标平移：

  ​		为了使多任务学习的目标函数和在下游任务做fine-tuning的目标函数一致，RecAdam提出了目标平移的技术。将多任务目标函数的系数$\lambda$替换为退火函数$\lambda(t)$，多任务学习的目标函数变为：

  ​																	$$loss_{M}=\lambda(t)loss_{T}+(1-\lambda(t))loss_{S}$$

  其中$t$为fine-tuning时候的步长，$\lambda(t)=\frac{1}{1+exp^{-k.(t-t_{0})}}$。在fine-tuning的时候好，即$\lambda(t)\rightarrow0$，即模型更关注从预训练模型学到的知识，随着训练的进行，$\lambda(t)\rightarrow1$，即模型更关注从下游任务中学到的知识。

  ​		通过预训练模拟和目标平移机制，RecAdam较好的缓解了退化遗忘的问题，使得对下游任务做fine-tuning的时候可以取得更好的结果。

### 2.2 实体识别

​		实体识别即识别出文本中的各种类型的实体，本方案基于FLAT及DiceLoss完成query中的实体识别。

- FLAT

- DiceLoss

  ​		DiceLoss又名骰子损失函数，在nlp的一些样本不均衡的任务中有比较好的效果。在实体识别任务中，由于大量的非实体词的存在，导致数据中实体词和非实体词的占比分布不均衡，DiceLoss的应用可以有效的提升实体识别的效果。

  ​		数据不均衡往往会产业两个问题，1. 训练-测试不一致性:在不均衡的数据集中训练模型，训练过程收敛的时候模型会严重倾向于占主体的标签，然而在预测的时候，F1值给予正负样本一样的权重；2. 简单负样本的主导影响: 在不均衡的数据集中，大量负样本的存在往往意味中存在大量的**简单负样本**，这些数量庞大的简单负样本往往会主导训练的过程，是模型不足以学会去区分正样本和**困难负样本**。

  ​		DiceLoss使用Dice系数来解决训练-预测不一致的问题。Dice系数是准确率和召回率的调和平均，它给予错误的正样本和错误的负样本一样的权重，因此对于数据不均衡的数据集更加适应。

### 2.3 实体链接

#### 2.3.1 实体指称生成

​		实体指称是指标准实体的一些别名以及简称等，例如对于标准实体"深圳市腾讯计算机系统有限公司"，又可以被称为"腾讯"、"鹅厂"等。**添加保安区相关示例**。因此对于图谱中的标准实体，通常需要生成与之相对应的指称，使实体识别模型提取出来的实体指称能对应到图谱中的标准实体上。					

- 公司实体提及生成

​		对于一个具体的公司名，通过分词模块可以将其分成四部分，即\[地理前缀\]-\[公司主体\]-\[公司标签\]-\[公司后缀]。取公司主体以及公司主体-公司标签作为公司实体的两个简称。对于"深圳市腾讯计算机系统有限公司"而言，其分词后的结果为\[深圳市\]-\[腾讯\]-\[计算机系统\]-\[有限公司\]。

​		地名和公司后缀可以通过地名词典和公司后缀词典以最大长度匹配的方式得到。对于剩余的中间部分"腾讯计算机系统"，可直接用来作为公司实体的一个简称。
​		除此之外，通过进一步的切分，可得到"腾讯"这样的公司主体简称，客户在搜索时，常常也是输入公司主体简称。对于"腾讯计算机系统"，首先基于常用的分词工具如jieba分词等，对其进行切分，可得到"腾讯"、"计算机"、"系统"等三个token，分别以顺序及倒序的方式计算token拼接后的概率，基于拼接后的概率确定公司主体以及公司标签部分。例如$P(腾讯,计算机)=0.000062$，$P(计算机,系统)=0.00023$，通过对比发现顺序拼接后的概率明显小于倒序拼接后的概率，因此"腾讯"被切分为公司主体，而计算机系统则被切分为公司标签。

- 地址实体提及生成

​		基于公司的寄递数据，可得到同一个公司名下日常寄递所使用的地址的不同写法(不同的人寄送物品时对地址的写法不一致)，再结合公司内部的地址分词服务，可以得到同一个地址不同层级的不同写法，这些不同的写法则可以作为地址实体的不同指代。

#### 2.3.2 实体召回

​		如果实体识别模型未能从query中抽取出实体或实体提及，这时就需要对query做切分，生成一批候选实体，计算这些候选实体与图谱中实体间的语意相似性，对候选实体做排序，再结合前面识别出的意图选择候选实体作为最终实体。

- 候选实体生成

  采用ngram穷举的方式生成候选实体。首先基于jieba等分词工具对query做切分，得到一系列token，然后给定不同的n值得到一批候选实体。如"南山区生态科技园有哪些高新技术企业"通过ngram穷举的方式可切分出"南山区生态"、"生态科技园"、"南山区生态科技园"、"高新技术"、"高新技术企业"、"企业"等候选实体。

- 候选实体匹配

  ​		对生成的候选实体做文本编码，然后计算候选实体编码与图谱中实体编码的相似度，取相似度最高的候选实体作为识别出的实体。基于SimCSE对候选实体和图谱中的实体做文本编码，然后计算两者之间的余弦相似度。

  ​		SimCSE可通过有监督及无监督的方式在给定的语料上做fine-tuning，然后给予fine-tuning完的模型可获取文本的嵌入表示。我们这里基于无监督的方式，基于人民日报和维基百科的中文语料来fine-tuning，然后给予fine-tuning的模型得到候选实体和图谱中标准实体的embedding。SimCSE的两种fine-tuning方式如下图所示：

  ​		**补充图片**

  ​		对于给定的一组文本，将**同一文本**分别做两次dropout，将两次dropout后的文本表示作为一组正样本；将**不同的一对文本**作为负样本。

  ​		SimCSE以无监督的方式fine-tuning的损失函数为：

  ​																		$$l_{i}=-log\frac{e^{sim(h_{i}^{z_{i}},h_{i}^{z_{i}^{’}})/\tau}}{\sum_{j=1}^{N}e^{sim(h_{i}^{z_{i}},h_{i}^{z_{j}^{’}})/\tau}}$$

  $h_{i}^{z_{i}}$，$h_{i}^{z_{i}^{'}}$分别为文本$z_{i}$经过两次dropout后获得的隐层表示；$h_{i}^{z_{i}}$，$h_{i}^{z_{j}^{'}}$分别为文本$z_{i}$和文本$z_{j}$的隐层表示；$N$为训练批次内文本数量。

  ​		整个实体召回的流程具体如下图所示：

  <img src="/Users/leotmc/Business Agendas/ft/entity_recall.png" alt="entity_recall" style="zoom:50%;" />

### 2.4 限定条件识别

​		对于"南山区生态科技园有哪些高新技术企业"而言，通过前面的实体识别以及意图分类，识别出的实体为："南山区生态科技园"与"企业"两个实体，识别出的意图为："找企业"，如果直接把这些信息形式化为查询语言去搜索企业就会造成搜索结果不够精准，因为客户输入了"高新技术"这样的限定条件。一般而言，有两种形式的限定条件：1. 属性值单独出现；2. 属性的键和值同时出现。

- 只有属性值的限定条件

  只有属性值的限定条件一般而言是针对一些枚举类型的属性信息，如"高新技术"这种标签类型的属性。针对这种情况具体做法是，在识别出实体后，基于图谱中实体的属性标签在query中做查询，命中的属性标签即被识别为限定条件。

- 属性键和值均有的限定条件

  对于"南山区生态科技园有哪些注册资金在1000万以上的企业"这样的query，其限定条件为注册资金 > 1000万，注册资金和1000万分别为属性的键和值，通过规则模版可识别query中的键和值。

在识别出query中的限定条件后，将其与意图、实体、属性等相结合，形成最终的搜索语句去做检索，可准确的查找到用户所需要的信息。

## 3. 图谱应用

